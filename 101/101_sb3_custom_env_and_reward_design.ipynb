{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X2jez89FUk_"
      },
      "source": [
        "# Notes\n",
        "\n",
        "MuJoCo environments: https://gymnasium.farama.org/environments/mujoco/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD7W5cyOEmjt"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekllGKpaxhgu"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "orm-hmh9xWqT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA_VTiZYI6vP"
      },
      "source": [
        "## Setup - Stable baseline and Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-YOdjdhOWft",
        "outputId": "77854d89-5ba8-4867-c5a4-316e62687af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0iYf6MyOiAL",
        "outputId": "25d8c7ef-9488-44db-d68b-149bca349399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q11TKKM1El_S"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import SAC, TD3, A2C\n",
        "import os\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF1fz53cB21g"
      },
      "outputs": [],
      "source": [
        "from gymnasium import Env, spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cwdwrFHEeIk"
      },
      "outputs": [],
      "source": [
        "#@title Create directories to hold models and logs\n",
        "model_dir = \"models\"\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGBmuh2DE6eB"
      },
      "source": [
        "# Training and testing function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heRI_ExiEtf5"
      },
      "outputs": [],
      "source": [
        "def train_model(env, sb3_algo, max_iter = 10, env_name = ''):\n",
        "    if sb3_algo=='SAC':\n",
        "        model = SAC('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "    elif sb3_algo=='TD3':\n",
        "        model = TD3('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "    elif sb3_algo=='A2C':\n",
        "        model = A2C('MlpPolicy', env, verbose=1, device='cuda', tensorboard_log=log_dir)\n",
        "    else:\n",
        "        print('Algorithm not found')\n",
        "        return\n",
        "\n",
        "    TIMESTEPS = 25000\n",
        "    iters = 0\n",
        "    while True:\n",
        "        iters += 1\n",
        "\n",
        "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
        "        model.save(f\"{model_dir}/{sb3_algo}_{env_name}_{TIMESTEPS*iters}\")\n",
        "\n",
        "        if iters>=max_iter:\n",
        "          break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOrir3cPEvk9"
      },
      "outputs": [],
      "source": [
        "def test_model(env, sb3_algo, path_to_model, extra_steps = 500):\n",
        "\n",
        "    if sb3_algo=='SAC':\n",
        "        model = SAC.load(path_to_model, env=env)\n",
        "    elif sb3_algo=='TD3':\n",
        "        model = TD3.load(path_to_model, env=env)\n",
        "    elif sb3_algo=='A2C':\n",
        "        model = A2C.load(path_to_model, env=env)\n",
        "    else:\n",
        "        print('Algorithm not found')\n",
        "        return\n",
        "\n",
        "    obs = env.reset()[0]\n",
        "    done = False\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, done, _, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            extra_steps -= 1\n",
        "\n",
        "            if extra_steps < 0:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8IQKl28S8xi"
      },
      "outputs": [],
      "source": [
        "def test_model_simple(env, sb3_algo, path_to_model, extra_steps = 0, seed = None):\n",
        "\n",
        "    if sb3_algo=='SAC':\n",
        "        model = SAC.load(path_to_model, env=env)\n",
        "    elif sb3_algo=='TD3':\n",
        "        model = TD3.load(path_to_model, env=env)\n",
        "    elif sb3_algo=='A2C':\n",
        "        model = A2C.load(path_to_model, env=env)\n",
        "    else:\n",
        "        print('Algorithm not found')\n",
        "        return\n",
        "\n",
        "    obs = env.reset(seed)\n",
        "    done = False\n",
        "    obs_list = [env.render(mode='rgb_array')]\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        obs_list.append(env.render(mode='rgb_array'))\n",
        "\n",
        "        if terminated or truncated:\n",
        "            extra_steps -= 1\n",
        "\n",
        "            if extra_steps < 0:\n",
        "                break\n",
        "    return obs_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfZ2JHeqBLt1"
      },
      "source": [
        "# Custom env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cxg0C-WMKaTt"
      },
      "outputs": [],
      "source": [
        "#@title simple_env_for_sbl3 [from:'101 Gym - Creating custom gym env.ipynb']\n",
        "\n",
        "class SimpleEnv4SBL3(Env):\n",
        "  def __init__(self,cnv_sz = 10,obs_factr=0.3,env_seed=0, rwd_choice = 0):\n",
        "    super(SimpleEnv4SBL3, self).__init__()\n",
        "    # Define a 2-D observation space\n",
        "    self.observation_shape = (2,)\n",
        "    self.observation_space = spaces.Box(low = np.zeros(self.observation_shape),\n",
        "                                        high = np.ones(self.observation_shape),\n",
        "                                        dtype = np.float16)\n",
        "\n",
        "    # Define an action space ranging from 0 to 4\n",
        "    action_shape = (1,)\n",
        "    self.action_space = spaces.Box(low = np.zeros(action_shape),\n",
        "                                        high = 4*np.ones(action_shape),\n",
        "                                        dtype = np.float16)\n",
        "\n",
        "    self.env_seed = env_seed\n",
        "    self.obs_factr = obs_factr\n",
        "    self.cnv_sz = cnv_sz\n",
        "    self.rwd_choice = rwd_choice\n",
        "\n",
        "    # Reset\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "\n",
        "    # Env\n",
        "    np.random.seed(self.env_seed)\n",
        "    #   - we will leave the bottom right corner for goal and keep it obstacle free\n",
        "    #   - self._goal_state = (self.cnv_sz-1,self.cnv_sz-1) # tuple(np.random.choice(np.arange(self.cnv_sz),2))\n",
        "    # > goal\n",
        "    if np.random.uniform(0,1)<=0.5: # 50% probability\n",
        "      self._goal_state = (self.cnv_sz-1, np.random.choice(np.arange(self.cnv_sz),1)[0]) # bottom goal\n",
        "    else:\n",
        "      self._goal_state = (np.random.choice(np.arange(self.cnv_sz),1)[0],self.cnv_sz-1) # right side goal\n",
        "    # > obstacles\n",
        "    all_states_except_few = [(i,j) for i in range(1,self.cnv_sz-1) for j in range(1,self.cnv_sz-1) \\\n",
        "                  if not (i,j) == self._goal_state] # all states except corner and goal (path exists)\n",
        "    total_obs_region = (self.cnv_sz-2)**2-2 # len(all_states_except_few)\n",
        "    no_of_obs = np.floor(self.obs_factr*total_obs_region)\n",
        "    all_states_except_few.pop(0) # <-- remove start point\n",
        "    sel_obs = np.random.choice( np.arange(total_obs_region) ,int(no_of_obs),replace=False)\n",
        "    self._obstacles = [all_states_except_few[i] for i in sel_obs]\n",
        "\n",
        "    # Episode settings\n",
        "    self._max_episode_steps = self.cnv_sz**2 # 2*self.cnv_sz\n",
        "\n",
        "    # State\n",
        "    if seed==0:\n",
        "      self.state = (0,0)\n",
        "    else:\n",
        "      np.random.seed(seed)\n",
        "      self.state = tuple(np.random.choice( np.arange(self.cnv_sz),2 ))\n",
        "\n",
        "    # Steps\n",
        "    self._elapsed_steps = 0\n",
        "\n",
        "    return self.state\n",
        "\n",
        "  def render(self, mode = \"human\"):\n",
        "    assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either \\\"human\\\" or \\\"rgb_array\\\"\"\n",
        "    cnv = np.zeros((self.cnv_sz,self.cnv_sz))\n",
        "    for i,j in self._obstacles:\n",
        "      cnv[i,j] = np.nan\n",
        "    cnv[self.state[0],self.state[1]] = 1\n",
        "    cnv[self._goal_state[0],self._goal_state[1]] = np.inf\n",
        "    if mode == \"human\":\n",
        "      print(f'INFO: The {np.nan} are obstacles and {np.inf} is the goal. Bot starts at (0,0).')\n",
        "      print(cnv)\n",
        "    elif mode == \"rgb_array\":\n",
        "      return cnv\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, action):\n",
        "    info = {}\n",
        "\n",
        "    if np.round(action)==0: # left\n",
        "      move = (0,-1)\n",
        "    elif np.round(action)==1: # right\n",
        "      move = (0,1)\n",
        "    elif np.round(action)==2: # up\n",
        "      move = (-1,0)\n",
        "    elif np.round(action)==3: # down\n",
        "      move = (1,0)\n",
        "    else:\n",
        "      move = (0,0)\n",
        "\n",
        "    self.state = (np.clip(self.state[0] + move[0], 0, self.cnv_sz-1), \\\n",
        "                  np.clip(self.state[1] + move[1], 0, self.cnv_sz-1) )\n",
        "\n",
        "    reward, terminated, truncated = self.reward_design()\n",
        "\n",
        "    return self.state, reward, terminated, truncated, info\n",
        "\n",
        "  def reward_design(self):\n",
        "    reward = 0.\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    if self.rwd_choice==0: # Exploratory to goal - takes longest path to goal\n",
        "      # If we know how many steps to optimally explore, we can put this number into 'self._max_episode_steps' for efficient exploration.\n",
        "      if self.state == self._goal_state:\n",
        "        reward = self.cnv_sz**2\n",
        "        terminated = True\n",
        "      else:\n",
        "        reward = 1.-np.linalg.norm(np.array(self.state)-np.array(self._goal_state))/self.cnv_sz # positive\n",
        "      for obs_state in self._obstacles:\n",
        "        if self.state == obs_state:\n",
        "          reward = -1.\n",
        "          terminated = True\n",
        "\n",
        "      self._elapsed_steps += 1\n",
        "      if self._elapsed_steps >= self._max_episode_steps:\n",
        "        truncated = True\n",
        "\n",
        "    elif self.rwd_choice==1: # Go fast to goal and stay (reward the approach and at goal)\n",
        "      if self.state == self._goal_state:\n",
        "        reward = self.cnv_sz**2\n",
        "      else:\n",
        "        reward = 1.-np.linalg.norm(np.array(self.state)-np.array(self._goal_state))/self.cnv_sz # positive\n",
        "      for obs_state in self._obstacles:\n",
        "        if self.state == obs_state:\n",
        "          reward = -1.\n",
        "          terminated = True\n",
        "\n",
        "      self._elapsed_steps += 1\n",
        "      if self._elapsed_steps >= self._max_episode_steps:\n",
        "        truncated = True\n",
        "\n",
        "    elif self.rwd_choice==2: # Go fast to goal and end (reward the approach <= penalize the time)\n",
        "      if self.state == self._goal_state:\n",
        "        reward = self.cnv_sz**2\n",
        "        terminated = True\n",
        "      else:\n",
        "        reward = -np.linalg.norm(np.array(self.state)-np.array(self._goal_state))/self.cnv_sz # negative\n",
        "      for obs_state in self._obstacles:\n",
        "        if self.state == obs_state:\n",
        "          reward = -self.cnv_sz**2. # should be large\n",
        "          terminated = True\n",
        "\n",
        "      self._elapsed_steps += 1\n",
        "      if self._elapsed_steps >= self._max_episode_steps:\n",
        "        truncated = True\n",
        "\n",
        "    return reward, terminated, truncated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOsFYQyP_3g5"
      },
      "source": [
        "# Training and Testing SBL3 on custom model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5GQv5qC83Sw"
      },
      "source": [
        "### ENV case 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QxCnLyrzKxd"
      },
      "outputs": [],
      "source": [
        "#@title Test env 1\n",
        "\n",
        "nwenv = SimpleEnv4SBL3()\n",
        "nwenv.reset()\n",
        "nwenv.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIL2OsJSIoPK"
      },
      "outputs": [],
      "source": [
        "#@title Test env 1 actions\n",
        "action_list1 = [3 for _ in range(nwenv.cnv_sz-1)] + [1 for _ in range(nwenv.cnv_sz-1)] \\\n",
        "              + [2 for _ in range(nwenv.cnv_sz-1)]\n",
        "action_list2 = np.random.choice(np.arange(4), 30)\n",
        "action_list3 = [1,3,0,2]*26\n",
        "action_sequences = {'go_goal':action_list1, 'go_random':action_list2, 'go_circle':action_list3}\n",
        "reward_dict = dict()\n",
        "for key in action_sequences.keys():\n",
        "  print(f'\\n<< Control sequence ID: {key} >>')\n",
        "  if key=='go_random':\n",
        "    nwenv.reset()\n",
        "    print('Start point: random')\n",
        "  else:\n",
        "    nwenv.reset(seed=0)\n",
        "    print('Start point: (0,0)')\n",
        "  rwd_seq = [0.]\n",
        "  cnv_traj = nwenv.render(mode='rgb_array')\n",
        "  for action in action_sequences[key]:\n",
        "    _,rwd,tnt,tnc,_ = nwenv.step( action )\n",
        "    rwd_seq.append(rwd_seq[-1] + rwd)\n",
        "    cnv = nwenv.render(mode='rgb_array')\n",
        "    cnv_traj += np.array(cnv)\n",
        "    if tnt:\n",
        "      print(f'The state is {nwenv.state} and the goal is {nwenv._goal_state}.')\n",
        "      if nwenv.state != nwenv._goal_state:\n",
        "        print('Fell into abyss!')\n",
        "      else:\n",
        "        print('Reached goal!')\n",
        "      break\n",
        "    if tnc:\n",
        "      print('Too many steps taken!')\n",
        "      break\n",
        "  reward_dict[key] = rwd_seq\n",
        "  print(cnv_traj)\n",
        "  plt.plot(rwd_seq,label=key)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8RwLx0xBZfJ"
      },
      "outputs": [],
      "source": [
        "gymenv = SimpleEnv4SBL3()\n",
        "SBL3_DRL_NAME = 'SAC' # TD3, etc.\n",
        "max_iter = 10\n",
        "train_model(gymenv, SBL3_DRL_NAME, max_iter = max_iter, env_name='SimpleEnv4SBL3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyRLMkSXKX5w"
      },
      "outputs": [],
      "source": [
        "cnv_list = test_model_simple(gymenv, SBL3_DRL_NAME, f'models/SAC_SimpleEnv4SBL3_{max_iter*25000}', seed=0)\n",
        "cnv_traj = np.array(cnv_list[0])\n",
        "for cnv in cnv_list[1:]:\n",
        "  cnv_traj += np.array(cnv)\n",
        "print(cnv_traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592JmfWL87Lr"
      },
      "source": [
        "### ENV case 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrqeRT8Z8-Xh"
      },
      "outputs": [],
      "source": [
        "#@title Test env 1\n",
        "\n",
        "nwenv = SimpleEnv4SBL3(cnv_sz=18,obs_factr=0.2,env_seed=1)\n",
        "nwenv.reset()\n",
        "nwenv.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryIW1APV9ET0"
      },
      "outputs": [],
      "source": [
        "gymenv = SimpleEnv4SBL3()\n",
        "SBL3_DRL_NAME = 'SAC' # TD3, etc.\n",
        "max_iter = 1000\n",
        "train_model(gymenv, SBL3_DRL_NAME, max_iter = max_iter, env_name='SimpleEnv4SBL3_case2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd3_BU6B9ET2"
      },
      "outputs": [],
      "source": [
        "cnv_list = test_model_simple(gymenv, SBL3_DRL_NAME, f'models/SAC_SimpleEnv4SBL3_{max_iter*25000}', seed=0)\n",
        "cnv_traj = np.array(cnv_list[0])\n",
        "for cnv in cnv_list[1:]:\n",
        "  cnv_traj += np.array(cnv)\n",
        "print(cnv_traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jZyqEqdyg6j"
      },
      "source": [
        "# Reward design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGm6Sf9s9v_p"
      },
      "source": [
        "### Reward type 2 - Go goal fast and stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clsdu1gJ9v_q"
      },
      "outputs": [],
      "source": [
        "#@title Test env 1\n",
        "\n",
        "gymenv = SimpleEnv4SBL3(cnv_sz=8,obs_factr=0.2,env_seed=1,rwd_choice=2)\n",
        "gymenv.reset()\n",
        "gymenv.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir2NNCGe9v_r"
      },
      "outputs": [],
      "source": [
        "SBL3_DRL_NAME = 'SAC' # TD3, etc.\n",
        "max_iter = 20\n",
        "train_model(gymenv, SBL3_DRL_NAME, max_iter = max_iter, env_name='SimpleEnv4SBL3_rwdtype2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOsCMzFg9v_r"
      },
      "outputs": [],
      "source": [
        "# saved_pathfile_name = 'models/SAC_SimpleEnv4SBL3_rwdtype2_500000'\n",
        "saved_pathfile_name = f'models/SAC_SimpleEnv4SBL3_rwdtype2_{max_iter*25000}'\n",
        "cnv_list = test_model_simple(gymenv, SBL3_DRL_NAME, saved_pathfile_name, seed=0)\n",
        "cnv_traj = np.array(cnv_list[0])\n",
        "for cnv in cnv_list[1:]:\n",
        "  cnv_traj += np.array(cnv)\n",
        "print(cnv_traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_52slGiBKew"
      },
      "source": [
        "# Ends"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QD7W5cyOEmjt",
        "ekllGKpaxhgu",
        "rOsFYQyP_3g5",
        "U5GQv5qC83Sw",
        "592JmfWL87Lr"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
